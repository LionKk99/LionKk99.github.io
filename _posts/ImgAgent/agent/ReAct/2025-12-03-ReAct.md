---
layout: post
title: "ReAct"
date: 2025-12-03 13:09:50 +0800
categories: ['ImgAgent', 'agent', 'ReAct']
tags: ['ImgAgent', 'agent']
image: "/images/ImgAgent/agent/ReAct/F1.png"
math: true
toc: true
---

![F1.png]({{ "/images/ImgAgent/agent/ReAct/F1.png" | absolute_url }})
### Relative Work
- [[Chain-of-Thought]]
### Intro
作者想要将LLMs的两个任务reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation)结合在一起，通过reasoning和acting的协同操作，提升能力和可解释性（e.g.通过维基百科api
交互，克服思维链中的幻觉问题）
```
ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).
```
### REACT: SYNERGIZING REASONING + ACTING
对于一个一般的智能体agent来说，其在时间t时，根据它当前的观察Ot以及之前的观察-动作序列，根据policy π 从动作空间A中采取一个动作at。但是这样的policy学习起来是十分困难的。
现在ReAct扩展agent的动作空间 $A* = A ∪ L$ ,$L$是一个语言空间。对于a∈L，a可以是一个 thought or reasoning trace，其不与外界环境交互，所以不会产生任何feedback。相反的，这个a的目的是通过reasoning整合有效的信息来支持下一步的操作，包括/分解任务目标并制定计划 / 获取外部额外信息 / 总结观察到的重要信息 / 跟踪进度并推进计划 / 处理意外情况并调整计划。（e.g. decomposing task goals and create action plans / injecting commonsense knowledge relevant to task solving / extracting important parts from observations / track progress and transit action plans / handle exceptions and adjust action plans）
&nbsp;
两种模式：
- 对于需要大量推理的任务，交替生成thoughts and actions
- 对于需要大量动作的任务，thoughts只需要稀疏的出现，让LLM自行决定调用
### Exp
![T1F2.png]({{ "/images/ImgAgent/agent/ReAct/T1F2.png" | absolute_url }})
文中额外对比了采取回退的方式，如
- ReAct → CoT-SC：当ReAct在给定步骤内未能返回答案时，回退到CoT-SC
- CoT-SC → ReAct：当n个CoT-SC样本中多数答案出现的次数少于n/2次时（即内部知识可能不足以可靠地支持该任务），回退到ReAct
![F3.png]({{ "/images/ImgAgent/agent/ReAct/F3.png" | absolute_url }})
文中指出相比之下，对于 PaLM8/62B 来说，对 Standard 或 CoT 进行微调明显不如对 ReAct 或 Act 进行微调，因为前者本质上是教模型记忆（可能是幻觉的）知识事实，而后者是教模型如何（推理和）行动以从维基百科获取信息，这是一种更具普遍性的知识推理技能。